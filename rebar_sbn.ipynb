{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of REBAR (https://arxiv.org/abs/1703.07370), a low-variance, unbiased gradient estimator for discrete latent variable models. This notebook is focused on the generative modeling experiments on the MNIST and Omniglot datasets from Section 5.2.1.\n",
    "\n",
    "The problem being solved is $\\text{max} \\hspace{5px} \\mathbb{E} [f(b, \\theta) | p(b) ]$, $b$ ~ Bernoulli($\\theta$).\n",
    "\n",
    "For generative modeling, the objective is to maximize a single-sample variational lower bound on the log-likelihood. There are two networks, one to model $q(b|x,\\theta)$ and one to model $p(x,b|\\theta)$. The former is the variational distribution and the latter is the joint probability distribution over the data and latent stochastic variables $b$.\n",
    "\n",
    "The **ELBO**, or evidence lower bound which we seek to maximize, is: \n",
    "\n",
    "$$\n",
    "\\log p(x \\vert \\theta) \\geq \\mathbb{E}_{q(b \\vert x,\\theta)} [ \\log p(x,b\\vert\\theta) - \\log q(b \\vert x,\\theta)]\n",
    "$$\n",
    "\n",
    "In practice, the Q-network has its own set of parameters $\\phi$ and the generator network $P$ has its own parameters $\\theta$.\n",
    "\n",
    "I'll refer to the learning signal $\\log p(x,b\\vert\\theta) - \\log q(b \\vert x,\\theta)$ as $l(x,b)$ for shorthand.\n",
    "\n",
    "The following is an implementation of a Sigmoid Belief Network (SBN) with REBAR gradient updates. I tried to follow the [author's TensorFlow implementation](https://github.com/tensorflow/models/blob/master/research/rebar/rebar.py) closely; there are a lot of computational statistics stuff going on that need to be implemented carefully.\n",
    "\n",
    "For an in-depth treatment on SBNs, see [this paper](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.63.1777&rep=rep1&type=pdf) by R. Neal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're just going to focus on the nonlinear SBN REBAR model.\n",
    "The model is pretty complex, so I'll implement it as separate modules and try to explain them\n",
    "one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import functools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import grad\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import rebar.datasets as datasets\n",
    "import rebar.util as U\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some global parameters we'll need later\n",
    "hparams = {\n",
    "    'model': 'SBNRebar',\n",
    "    'learning_rate':3e-4,\n",
    "    'n_hidden':200,\n",
    "    'n_input':784,\n",
    "    'temperature':0.5,\n",
    "    'eta':1.0,\n",
    "    'batch_size':24,\n",
    "    'task':'sbn',\n",
    "    'n_layers': 2,\n",
    "    'dynamic_b': False,\n",
    "    'ema_beta': 0.999,\n",
    "    'train_steps': 20000000,\n",
    "    'log_every': 500\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define samplers for producing the \"hard\" and \"soft\" reparameterized samples needed for computing the REBAR gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sample(log_alpha, u, layer, uniform_samples_v, temperature=None):\n",
    "    \"\"\"Returns sampled random variables parameterized by log_alpha.\"\"\"\n",
    "    # Generate tied randomness for later\n",
    "    if layer not in uniform_samples_v:\n",
    "        uniform_samples_v[layer] = u_to_v(log_alpha, u)\n",
    "        \n",
    "    # Sample random variable underlying softmax/argmax\n",
    "    x = log_alpha + U.safe_log_prob(u) - U.safe_log_prob(1 - u)\n",
    "    samples = ((x > 0).float()).detach()\n",
    "\n",
    "    return {\n",
    "        'preactivation': x,\n",
    "        'activation': samples,\n",
    "        'log_param': log_alpha,\n",
    "    }, uniform_samples_v\n",
    "\n",
    "def random_sample_soft(log_alpha, u, layer, uniform_samples_v, temperature=None):\n",
    "    \"\"\"Returns sampled random variables parameterized by log_alpha.\"\"\"\n",
    "\n",
    "    # Sample random variable underlying softmax/argmax\n",
    "    x = log_alpha + U.safe_log_prob(u) - U.safe_log_prob(1 - u)\n",
    "    x = x.clone() / temperature.view(-1)\n",
    "    y = F.sigmoid(x)\n",
    "\n",
    "    return {\n",
    "        'preactivation': x,\n",
    "        'activation': y,\n",
    "        'log_param': log_alpha\n",
    "    }, uniform_samples_v\n",
    "\n",
    "def random_sample_soft_v(log_alpha, _, layer, uniform_samples_v, temperature=None):\n",
    "    \"\"\"Returns sampled random variables parameterized by log_alpha.\"\"\"\n",
    "    v = uniform_samples_v[layer]\n",
    "    return random_sample_soft(log_alpha, v, layer, uniform_samples_v, temperature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next bit, for producing common random numbers, is for variance reduction. [The general idea behind common random numbers is easy enough to grasp](https://en.wikipedia.org/wiki/Variance_reduction), but what the authors are doing here is a bit more subtle. According to Appendix G.2, they're correlating u and v to reduce the variance of the gradient by first sampling u and then using that to determine v."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Random samplers TODO\n",
    "def u_to_v(log_alpha, u, eps = 1e-8):\n",
    "    \"\"\"Convert u to tied randomness in v.\"\"\"\n",
    "    u_prime = F.sigmoid(-log_alpha)  # g(u') = 0\n",
    "    v_1 = (u - u_prime) / torch.clamp(1 - u_prime, eps, 1)\n",
    "    v_1 = torch.clamp(v_1.clone(), 0, 1).detach()\n",
    "    v_1 = v_1.clone()*(1 - u_prime) + u_prime\n",
    "    v_0 = u / torch.clamp(u_prime, eps, 1)\n",
    "    v_0 = torch.clamp(v_0.clone(), 0, 1).detach()\n",
    "    v_0 = v_0.clone() * u_prime\n",
    "    v = u.clone()\n",
    "    v[(u > u_prime).detach()] = v_1\n",
    "    v[(u <= u_prime).detach()] = v_0\n",
    "    # TODO: add pytorch check\n",
    "    #v = tf.check_numerics(v, 'v sampling is not numerically stable.')\n",
    "    vv = v + (-v + u).detach()  # v and u are the same up to numerical errors\n",
    "    return Variable(vv.data, requires_grad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the deterministic mapping we'll use to construct the stochastic layers of the Q- and P-networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformation(nn.Module):\n",
    "    \"\"\"\n",
    "    Deterministic transformation between stochastic layers\n",
    "    \n",
    "        x -> FC -> Tanh -> FC -> Tanh() -> FC -> logQ\n",
    "            \n",
    "    \"\"\"\n",
    "    def __init__(self, n_input, n_hidden, n_output):\n",
    "        super(Transformation, self).__init__()\n",
    "        self.h = nn.Sequential(\n",
    "            nn.Linear(n_input, n_hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(n_hidden, n_output))\n",
    "        \n",
    "        for layer in self.h:\n",
    "            if hasattr(layer, 'weight'):\n",
    "                U.scaled_variance_init(layer)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        return self.h(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RecognitionNet is the variational distribution (Q-network) and the GeneratorNet is the joint distribution of the data and latent variables (P-network). It looks like this for an unrolled 2-layer SBN, where Sample is the stochastic layer of Bernoulli units:\n",
    "\n",
    "// Replace with figure?\n",
    "\n",
    "x -> Transformation(x) -> Sample(x) -> Transformation(x) -> Sample(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecognitionNet(nn.Module):\n",
    "    \"\"\"\n",
    "    given x values, samples from Q and returns log Q(h|x)\n",
    "    \"\"\"\n",
    "    def __init__(self, mean_xs, sampler):\n",
    "        super(RecognitionNet, self).__init__()\n",
    "        self.mean_xs = mean_xs\n",
    "        self.sampler = sampler\n",
    "        self.transforms = nn.ModuleList([Transformation(hparams['n_input'],\n",
    "                                        hparams['n_hidden'], hparams['n_hidden'])])\n",
    "        if hparams['n_layers'] > 1:\n",
    "            for _ in range(1, hparams['n_layers']):\n",
    "                self.transforms.append(Transformation(hparams['n_hidden'], hparams['n_hidden'],\n",
    "                                                     hparams['n_hidden']))\n",
    "        self.uniform_samples = dict()\n",
    "        self.uniform_samples_v = dict()\n",
    "        # generate randomness\n",
    "        for i in range(hparams['n_layers']):\n",
    "            self.uniform_samples[i] = Variable(\n",
    "                torch.FloatTensor(hparams['batch_size'], hparams['n_hidden']).uniform_(0,1),\n",
    "                requires_grad=False)\n",
    "            \n",
    "    def forward(self, x, sampler_=None):\n",
    "        if sampler_ is not None:\n",
    "            sampler = sampler_\n",
    "        else:\n",
    "            sampler = self.sampler\n",
    "        samples = {}\n",
    "        samples[-1] = {'activation': x}\n",
    "        # center the input\n",
    "        samples[-1]['activation'] = samples[-1]['activation'].clone() - self.mean_xs\n",
    "        samples[-1]['activation'] = (samples[-1]['activation'].clone() + 1)/2.\n",
    "        logQ = []\n",
    "        logitss = []\n",
    "        for i,t in enumerate(self.transforms):\n",
    "            input = 2 * samples[i-1]['activation'] - 1.0\n",
    "            logits = t(input)\n",
    "            logitss.append(logits)\n",
    "            # expect sampler to return a dictionary with key 'activation'\n",
    "            samples[i], self.uniform_samples_v = sampler(logits, self.uniform_samples[i],\n",
    "                                                         i, self.uniform_samples_v)\n",
    "            logQ.append(U.binary_log_likelihood(samples[i]['activation'], logits))  \n",
    "        # logQHard, samples\n",
    "        return logQ, samples, logitss\n",
    "\n",
    "class GeneratorNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Returns learning signal and function. Reconstructs the input.\n",
    "    \"\"\"\n",
    "    def __init__(self, mean_xs):\n",
    "        super(GeneratorNet, self).__init__()\n",
    "        self.transforms = []\n",
    "        for i in range(hparams['n_layers']):\n",
    "            if i == 0:\n",
    "                n_output = hparams['n_input']\n",
    "            else:\n",
    "                n_output = hparams['n_hidden']\n",
    "            self.transforms.append(Transformation(hparams['n_hidden'],\n",
    "                                                 hparams['n_hidden'], n_output))\n",
    "        self.transforms = nn.ModuleList(self.transforms)\n",
    "        self.prior = nn.Parameter(torch.zeros(hparams['n_hidden']))\n",
    "        self.train_bias = -np.log(1./np.clip(mean_xs.data.numpy(), 0.001, 0.999)-1.).astype(np.float32)\n",
    "        self.train_bias = Variable(torch.from_numpy(self.train_bias).float(), requires_grad=False)\n",
    "        \n",
    "    def forward(self, x, samples, logQ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            samples: dictionary of sampled latent variables\n",
    "            logQ: list of log q(h_i) terms\n",
    "        \"\"\"\n",
    "        sum_logQ = torch.sum(torch.stack(logQ), 0)\n",
    "        logPPrior = U.binary_log_likelihood(samples[hparams['n_layers']-1]['activation'], self.prior)\n",
    "        for i in reversed(range(hparams['n_layers'])):\n",
    "            # Set up the input to the layer\n",
    "            input = 2 * samples[i]['activation'] - 1.0\n",
    "            h = self.transforms[i](input)\n",
    "            if i == 0:\n",
    "                logP = U.binary_log_likelihood(x, h + self.train_bias)\n",
    "            else:\n",
    "                logPPrior = logPPrior.clone() + U.binary_log_likelihood(samples[i-1]['activation'], h)\n",
    "        # Note that logP(x,b) = logP(b|x) + logP(x)\n",
    "        # reinforce_learning_signal (l(x,b)), reinforce_model_grad\n",
    "        debug = {\n",
    "            'logP': logP,\n",
    "            'logPPrior': logPPrior,\n",
    "            'samples': samples\n",
    "        }\n",
    "        return logP + logPPrior - sum_logQ, logP + logPPrior, debug         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can put these modules together inside the SBNRebar module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SBNRebar(nn.Module):\n",
    "    def __init__(self, mean_xs):\n",
    "        super(SBNRebar, self).__init__()\n",
    "        self.mean_xs = mean_xs   \n",
    "        self._temperature = Variable(torch.FloatTensor([hparams['temperature']]), requires_grad=False)\n",
    "        self.recognition_network = RecognitionNet(mean_xs, random_sample)\n",
    "        self.generator_network = GeneratorNet(mean_xs) \n",
    "        self.eta = Variable(torch.FloatTensor([hparams['eta']]), requires_grad=False)\n",
    "                \n",
    "    def multiply_by_eta(self, grads):\n",
    "        res = []\n",
    "        for g in grads:\n",
    "            res.append(g*self.eta)\n",
    "        return res\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        All of the passes through the Q- and P-networks are here\n",
    "        \"\"\"\n",
    "        ###################################\n",
    "        # REINFORCE step (compute ELBO, etc.)\n",
    "        ###################################\n",
    "        # hardELBO is the non-differentiable learning signal, l(x,b)\n",
    "        #\n",
    "        # reinforce_model_grad is the joint distribution of interest p(x,b,\\theta), \n",
    "        #   and the gradient of l(x,b) wrt the P-network parameters is grad E[logP + logPPrior]  \n",
    "        #   = grad E[reinforce_model_grad]\n",
    "        # \n",
    "        # See https://github.com/tensorflow/models/blob/master/research/rebar/rebar.py#L716\n",
    "        logQHard, hardSamples, logits = self.recognition_network(x)\n",
    "        hardELBO, reinforce_model_grad, debug = self.generator_network(x, hardSamples, logQHard)\n",
    "        \n",
    "        ###################################\n",
    "        # compute Gumbel control variate\n",
    "        ###################################\n",
    "        # See https://github.com/tensorflow/models/blob/master/research/rebar/rebar.py#L659\n",
    "        logQ, softSamples, _ = self.recognition_network(x, sampler_=functools.partial(\n",
    "            random_sample_soft, temperature=self._temperature))\n",
    "        softELBO, _, _ = self.generator_network(x, softSamples, logQ)\n",
    "        # compute softELBO_v (same value as softELBO, different grads) :- zsquiggle = g(v, b, \\theta)\n",
    "        # NOTE: !!! Because of the common random numbers (u_to_v), z is distributed as z|b. \n",
    "        # So the reparameterization for p(z|b) is just g(v,b,\\theta) == g(v,\\theta) == log(\\theta/1-\\theta) + log(v/1-v)\n",
    "        # This is why random_sample_soft_v() just calls random_sample_soft(). I'm 95% sure this is correct...        \n",
    "        logQ_v, softSamples_v, _ = self.recognition_network(x, sampler_=functools.partial(\n",
    "            random_sample_soft_v, temperature=self._temperature))\n",
    "        # should be the same value as softELBO but different grads\n",
    "        softELBO_v, _, _ = self.generator_network(x, softSamples_v, logQ_v)\n",
    "        gumbel_cv_learning_signal = softELBO_v.detach()\n",
    "        gumbel_cv = gumbel_cv_learning_signal * torch.sum(torch.stack(logQHard), 0) - softELBO + softELBO_v\n",
    "\n",
    "        return {\n",
    "            'logQHard': logQHard,\n",
    "            'hardELBO': hardELBO,\n",
    "            'reinforce_model_grad': reinforce_model_grad,\n",
    "            'gumbel_cv': gumbel_cv,\n",
    "            'logits': logits,\n",
    "            'hardSamples': hardSamples,\n",
    "            'debug': debug\n",
    "        }   \n",
    "\n",
    "class Baseline(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Baseline, self).__init__()\n",
    "        # For centering the learning signal, from the NVIL paper (2.3.1) https://arxiv.org/pdf/1402.0030.pdf\n",
    "        # Input dependent baseline that is trained to minimize the MSE with the learning signal\n",
    "        self.out = nn.Sequential(\n",
    "           nn.Linear(hparams['n_input'], 100),\n",
    "           nn.Tanh(),\n",
    "           nn.Linear(100, 1))\n",
    "    \n",
    "        for layer in self.out:\n",
    "            if hasattr(layer, 'weight'):\n",
    "                U.scaled_variance_init(layer)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        return self.out(x).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random seed\n",
    "random_seed = 1337\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "# Load MNIST dataset\n",
    "train_xs, val_xs, test_xs = datasets.load_data(hparams)\n",
    "# create Dataloader\n",
    "train_dataloader = DataLoader(train_xs, shuffle=True, batch_size=hparams['batch_size'], drop_last=True, num_workers=0)\n",
    "# mean centering on training data\n",
    "mean_xs = Variable(torch.from_numpy(np.mean(train_xs, axis=0)).float(), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbn = SBNRebar(mean_xs)\n",
    "baseline = Baseline()\n",
    "baseline_loss = nn.MSELoss()\n",
    "sbn_opt = optim.Adam(sbn.parameters(), lr=hparams['learning_rate'], betas=(0.9, 0.99999))\n",
    "baseline_opt = optim.Adam(baseline.parameters(), lr=10*hparams['learning_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main training loop, where we compute REBAR gradients and update model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                         | 0/2083 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-20-36954433371e>(40)<module>()\n",
      "-> for (g, p) in zip(total_grads, sbn.parameters()):\n",
      "(Pdb) c\n",
      "torch.Size([200, 784]) -0.0029144370928406715 0.17245738208293915\n",
      "torch.Size([200]) 0.030689282342791557 2.3916244506835938\n",
      "torch.Size([200, 200]) -0.0028852615505456924 0.1588493436574936\n",
      "torch.Size([200]) 0.1195976585149765 1.4188839197158813\n",
      "torch.Size([200, 200]) -0.0021161925978958607 0.15428811311721802\n",
      "torch.Size([200]) 0.023251790553331375 0.9594911336898804\n",
      "torch.Size([200, 200]) -0.0010304829338565469 0.6176221370697021\n",
      "torch.Size([200]) -0.051269032061100006 0.7328332662582397\n",
      "torch.Size([200, 200]) -6.581057095900178e-05 0.41514357924461365\n",
      "torch.Size([200]) 0.01984141767024994 0.9645709991455078\n",
      "torch.Size([200, 200]) 0.002894527046009898 0.3417595624923706\n",
      "torch.Size([200]) 0.11603332310914993 0.982020914554596\n",
      "torch.Size([200]) -0.007708342280238867 0.011430026032030582\n",
      "torch.Size([200, 200]) -0.00023348425747826695 0.010858419351279736\n",
      "torch.Size([200]) 0.005929088220000267 0.011689562350511551\n",
      "torch.Size([200, 200]) -0.00037560894270427525 0.00836861040443182\n",
      "torch.Size([200]) -6.861924339318648e-05 0.016777684912085533\n",
      "torch.Size([784, 200]) -0.00020564570149872452 0.0017731859115883708\n",
      "torch.Size([784]) 0.01809697039425373 0.004389435984194279\n",
      "torch.Size([200, 200]) 3.060361632378772e-05 0.0074682096019387245\n",
      "torch.Size([200]) 0.0023537736851722 0.006652343086898327\n",
      "torch.Size([200, 200]) -0.0005735119339078665 0.006176134571433067\n",
      "torch.Size([200]) 0.0020444905385375023 0.010446916334331036\n",
      "torch.Size([200, 200]) 0.0001745870686136186 0.005243641324341297\n",
      "torch.Size([200]) 0.002823401475325227 0.011636696755886078\n",
      "step: 0.0, training objective (ELBO): -267.9793701171875, logGradVar: -8.899022102355957\n",
      "grad ema first moment: -8.225110263992974e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                 | 1/2083 [00:01<56:27,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-20-36954433371e>(39)<module>()\n",
      "-> pdb.set_trace()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Exception in thread Thread-21:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\pemami\\Continuum\\anaconda3\\lib\\threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\pemami\\Continuum\\anaconda3\\lib\\site-packages\\tqdm\\_tqdm.py\", line 148, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"C:\\Users\\pemami\\Continuum\\anaconda3\\lib\\_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exponential Moving Average for log variance calculation\n",
    "ema_first_moment = 0.\n",
    "ema_second_moment = 0.\n",
    "beta = hparams['ema_beta']\n",
    "log_every = hparams['log_every']\n",
    "n = hparams['train_steps']\n",
    "    \n",
    "step = 0.\n",
    "while step < n:\n",
    "    for x in tqdm(train_dataloader):\n",
    "        x = Variable(x, requires_grad=False)\n",
    "        sbn_outs = sbn.forward(x)\n",
    "        baseline_out = baseline.forward(x)\n",
    "        \n",
    "        nvil_gradient = (sbn_outs['hardELBO'].detach() - baseline_out) * \\\n",
    "            torch.sum(torch.stack(sbn_outs['logQHard']), 0) + sbn_outs['reinforce_model_grad']\n",
    "\n",
    "        f_grads = grad(-nvil_gradient.mean(), sbn.parameters(), retain_graph=True)\n",
    "        gumbel_grads = grad(sbn_outs['gumbel_cv'].mean(), sbn.parameters())\n",
    "        h_grads = sbn.multiply_by_eta(gumbel_grads)\n",
    "        total_grads = [(g_a + g_b) for (g_a, g_b) in zip(f_grads, h_grads)]\n",
    "\n",
    "        # training objective\n",
    "        lhat = sbn_outs['hardELBO'].mean().detach()\n",
    "\n",
    "        # baseline loss\n",
    "        baseline_y = baseline_loss(baseline_out, sbn_outs['hardELBO'].detach())\n",
    "\n",
    "        # variance summaries\n",
    "        first_moment = U.vectorize(total_grads, skip_none=True)\n",
    "        second_moment = first_moment ** 2\n",
    "        ema_first_moment = (beta * ema_first_moment) + (1 - beta) * first_moment\n",
    "        ema_second_moment = (beta * ema_second_moment) + (1 - beta) * second_moment\n",
    "        log_grad_variance = torch.log((ema_second_moment.mean() - (ema_first_moment.mean()) ** 2))\n",
    "\n",
    "        sbn_opt.zero_grad()\n",
    "        baseline_opt.zero_grad()\n",
    "\n",
    "        # set model grads with REBAR gradients\n",
    "        pdb.set_trace()                    \n",
    "        for (g, p) in zip(total_grads, sbn.parameters()):\n",
    "            print(g.shape, g.mean().data[0], g.var().data[0])\n",
    "            p.grad = g\n",
    "\n",
    "        sbn_opt.step()\n",
    "        # update baseline\n",
    "        baseline_y.backward()\n",
    "        baseline_opt.step()\n",
    "\n",
    "        if step % log_every == 0: \n",
    "            print('step: {}, training objective (ELBO): {}, logGradVar: {}'.format(step, lhat.data[0], log_grad_variance.data[0]))\n",
    "            print('grad ema first moment: {}'.format(ema_first_moment.mean().data[0]))\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
