{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of REBAR (https://arxiv.org/abs/1703.07370), a low-variance, unbiased gradient estimator for discrete latent variable models. This notebook is focused on the generative modeling experiments on the MNIST and Omniglot datasets from Section 5.2.1.\n",
    "\n",
    "The problem being solved is $\\text{max} \\hspace{5px} \\mathbb{E} [f(b, \\theta) | p(b) ]$, $b$ ~ Bernoulli($\\theta$).\n",
    "\n",
    "For generative modeling, the objective is to maximize a single-sample variational lower bound on the log-likelihood. There are two networks, one to model $q(b|x,\\theta)$ and one to model $p(x,b|\\theta)$. The former is the variational distribution and the latter is the joint probability distribution over the data and latent stochastic variables $b$.\n",
    "\n",
    "The **ELBO**, or evidence lower bound which we seek to maximize, is: \n",
    "\n",
    "$$\n",
    "\\log p(x \\vert \\theta) \\geq \\mathbb{E}_{q(b \\vert x,\\theta)} [ \\log p(x,b\\vert\\theta) - \\log q(b \\vert x,\\theta)]\n",
    "$$\n",
    "\n",
    "In practice, the Q-network has its own set of parameters $\\phi$ and the generator network $P$ has its own parameters $\\theta$.\n",
    "\n",
    "I'll refer to the learning signal $\\log p(x,b\\vert\\theta) - \\log q(b \\vert x,\\theta)$ as $l(x,b)$ for shorthand.\n",
    "\n",
    "The following is an implementation of a Sigmoid Belief Network (SBN) with REBAR gradient updates. I tried to follow the [author's TensorFlow implementation](https://github.com/tensorflow/models/blob/master/research/rebar/rebar.py) closely; there are a lot of computational statistics stuff going on that need to be implemented carefully.\n",
    "\n",
    "For an in-depth treatment on SBNs, see [this paper](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.63.1777&rep=rep1&type=pdf) by R. Neal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're just going to focus on the nonlinear SBN REBAR model.\n",
    "The model is pretty complex, so I'll implement it as separate modules and try to explain them\n",
    "one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import functools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import grad\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import rebar.datasets as datasets\n",
    "import rebar.util as U\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some global parameters we'll need later\n",
    "hparams = {\n",
    "    'model': 'SBNGumbel',\n",
    "    'learning_rate':3e-4,\n",
    "    'n_hidden':200,\n",
    "    'n_input':784,\n",
    "    'temperature':0.5,\n",
    "    'eta':0.1,\n",
    "    'batch_size':24,\n",
    "    'task':'sbn',\n",
    "    'n_layers': 2,\n",
    "    'dynamic_b': False,\n",
    "    'ema_beta': 0.999,\n",
    "    'train_steps': 20000000,\n",
    "    'log_every': 500\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define samplers for producing the \"hard\" and \"soft\" reparameterized samples needed for computing the REBAR gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sample(log_alpha, u, layer, uniform_samples_v, temperature=None):\n",
    "    \"\"\"Returns sampled random variables parameterized by log_alpha.\"\"\"\n",
    "    # Generate tied randomness for later\n",
    "    if layer not in uniform_samples_v:\n",
    "        uniform_samples_v[layer] = u_to_v(log_alpha, u)\n",
    "        \n",
    "    # Sample random variable underlying softmax/argmax\n",
    "    x = log_alpha + U.safe_log_prob(u) - U.safe_log_prob(1 - u)\n",
    "    samples = ((x > 0).float()).detach()\n",
    "\n",
    "    return {\n",
    "        'preactivation': x,\n",
    "        'activation': samples,\n",
    "        'log_param': log_alpha,\n",
    "    }, uniform_samples_v\n",
    "\n",
    "def random_sample_soft(log_alpha, u, layer, uniform_samples_v, temperature=None):\n",
    "    \"\"\"Returns sampled random variables parameterized by log_alpha.\"\"\"\n",
    "\n",
    "    # Sample random variable underlying softmax/argmax\n",
    "    x = log_alpha + U.safe_log_prob(u) - U.safe_log_prob(1 - u)\n",
    "    x = x.clone() / temperature.view(-1)\n",
    "    y = F.sigmoid(x)\n",
    "\n",
    "    return {\n",
    "        'preactivation': x,\n",
    "        'activation': y,\n",
    "        'log_param': log_alpha\n",
    "    }, uniform_samples_v\n",
    "\n",
    "def random_sample_soft_v(log_alpha, _, layer, uniform_samples_v, temperature=None):\n",
    "    \"\"\"Returns sampled random variables parameterized by log_alpha.\"\"\"\n",
    "    v = uniform_samples_v[layer]\n",
    "    return random_sample_soft(log_alpha, v, layer, uniform_samples_v, temperature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next bit, for producing common random numbers, is for variance reduction. [The general idea behind common random numbers is easy enough to grasp](https://en.wikipedia.org/wiki/Variance_reduction), but what the authors are doing here is a bit more subtle. According to Appendix G.2, they're correlating u and v to reduce the variance of the gradient by first sampling u and then using that to determine v."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Random samplers TODO\n",
    "def u_to_v(log_alpha, u, eps = 1e-8):\n",
    "    \"\"\"Convert u to tied randomness in v.\"\"\"\n",
    "    u_prime = F.sigmoid(-log_alpha)  # g(u') = 0\n",
    "    v_1 = (u - u_prime) / torch.clamp(1 - u_prime, eps, 1)\n",
    "    v_1 = torch.clamp(v_1.clone(), 0, 1).detach()\n",
    "    v_1 = v_1.clone()*(1 - u_prime) + u_prime\n",
    "    v_0 = u / torch.clamp(u_prime, eps, 1)\n",
    "    v_0 = torch.clamp(v_0.clone(), 0, 1).detach()\n",
    "    v_0 = v_0.clone() * u_prime\n",
    "    v = u.clone()\n",
    "    v[(u > u_prime).detach()] = v_1\n",
    "    v[(u <= u_prime).detach()] = v_0\n",
    "    # TODO: add pytorch check\n",
    "    #v = tf.check_numerics(v, 'v sampling is not numerically stable.')\n",
    "    vv = v + (-v + u).detach()  # v and u are the same up to numerical errors\n",
    "    return Variable(vv.data, requires_grad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the deterministic mapping we'll use to construct the stochastic layers of the Q- and P-networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformation(nn.Module):\n",
    "    \"\"\"\n",
    "    Deterministic transformation between stochastic layers\n",
    "    \n",
    "        x -> FC -> Tanh -> FC -> Tanh() -> FC -> logQ\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, n_input, n_hidden, n_output):\n",
    "        super(Transformation, self).__init__()\n",
    "        self.h = nn.Sequential(\n",
    "            nn.Linear(n_input, n_hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(n_hidden, n_output))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.h(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RecognitionNet is the variational distribution (Q-network) and the GeneratorNet is the joint distribution of the data and latent variables (P-network). It looks like this for an unrolled 2-layer SBN, where Sample is the stochastic layer of Bernoulli units:\n",
    "\n",
    "// Replace with figure?\n",
    "\n",
    "x -> Transformation(x) -> Sample(x) -> Transformation(x) -> Sample(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecognitionNet(nn.Module):\n",
    "    \"\"\"\n",
    "    given x values, samples from Q and returns log Q(h|x)\n",
    "    \"\"\"\n",
    "    def __init__(self, mean_xs, sampler):\n",
    "        super(RecognitionNet, self).__init__()\n",
    "        self.mean_xs = mean_xs\n",
    "        self.sampler = sampler\n",
    "        self.transforms = nn.ModuleList([Transformation(hparams['n_input'],\n",
    "                                        hparams['n_hidden'], hparams['n_hidden'])])\n",
    "        if hparams['n_layers'] > 1:\n",
    "            for _ in range(1, hparams['n_layers']):\n",
    "                self.transforms.append(Transformation(hparams['n_hidden'], hparams['n_hidden'],\n",
    "                                                     hparams['n_hidden']))\n",
    "        self.uniform_samples = dict()\n",
    "        self.uniform_samples_v = dict()\n",
    "        # generate randomness\n",
    "        for i in range(hparams['n_layers']):\n",
    "            self.uniform_samples[i] = Variable(\n",
    "                torch.FloatTensor(hparams['batch_size'], hparams['n_hidden']).uniform_(0,1),\n",
    "                requires_grad=False)\n",
    "            \n",
    "    def forward(self, x, sampler_=None):\n",
    "        if sampler_ is not None:\n",
    "            sampler = sampler_\n",
    "        else:\n",
    "            sampler = self.sampler\n",
    "        samples = {}\n",
    "        samples[-1] = {'activation': x}\n",
    "        samples[-1]['activation'] = samples[-1]['activation'].clone() - self.mean_xs\n",
    "        samples[-1]['activation'] = (samples[-1]['activation'].clone() + 1)/2.\n",
    "        logQ = []\n",
    "        for i,t in enumerate(self.transforms):\n",
    "            input = 2 * samples[i-1]['activation'] - 1.0\n",
    "            logits = t(input)\n",
    "            # expect sampler to return a dictionary with key 'activation'\n",
    "            samples[i], self.uniform_samples_v = sampler(logits, self.uniform_samples[i],\n",
    "                                                         i, self.uniform_samples_v)\n",
    "            logQ.append(U.binary_log_likelihood(samples[i]['activation'], logits))  \n",
    "        # logQHard, samples\n",
    "        return logQ, samples\n",
    "\n",
    "class GeneratorNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Returns learning signal and function. Reconstructs the input.\n",
    "    \"\"\"\n",
    "    def __init__(self, mean_xs):\n",
    "        super(GeneratorNet, self).__init__()\n",
    "        self.transforms = []\n",
    "        for i in range(hparams['n_layers']):\n",
    "            if i == 0:\n",
    "                n_output = hparams['n_input']\n",
    "            else:\n",
    "                n_output = hparams['n_hidden']\n",
    "            self.transforms.append(Transformation(hparams['n_hidden'],\n",
    "                                                 hparams['n_hidden'], n_output))\n",
    "        self.transforms = nn.ModuleList(self.transforms)\n",
    "        self.prior = Variable(torch.zeros(hparams['n_hidden']), requires_grad=False)\n",
    "        self.train_bias = -np.log(1./np.clip(mean_xs.data.numpy(), 0.001, 0.999)-1.).astype(np.float32)\n",
    "        self.train_bias = Variable(torch.from_numpy(self.train_bias).float(), requires_grad=False)\n",
    "        \n",
    "    def forward(self, x, samples, logQ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            samples: dictionary of sampled latent variables\n",
    "            logQ: list of log q(h_i) terms\n",
    "        \"\"\"\n",
    "        sum_logQ = torch.sum(torch.stack(logQ), 0)\n",
    "        logPPrior = U.binary_log_likelihood(samples[hparams['n_layers']-1]['activation'], self.prior)\n",
    "        for i in reversed(range(hparams['n_layers'])):\n",
    "            # Set up the input to the layer\n",
    "            input = 2 * samples[i]['activation'] - 1.0\n",
    "            h = self.transforms[i](input)\n",
    "            if i == 0:\n",
    "                logP = U.binary_log_likelihood(x, h + self.train_bias)\n",
    "            else:\n",
    "                logPPrior = logPPrior.clone() + U.binary_log_likelihood(samples[i-1]['activation'], h)\n",
    "        # Note that logP(x,b) = logP(b|x) + logP(x)\n",
    "        # reinforce_learning_signal (l(x,b)), reinforce_model_grad\n",
    "        return logP + logPPrior - sum_logQ, logP + logPPrior         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can put these modules together inside the SBNRebar module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SBNRebar(nn.Module):\n",
    "    def __init__(self, mean_xs):\n",
    "        super(SBNRebar, self).__init__()\n",
    "        self.mean_xs = mean_xs   \n",
    "        self._temperature = Variable(torch.FloatTensor([hparams['temperature']]), requires_grad=False)\n",
    "        self.recognition_network = RecognitionNet(mean_xs, random_sample)\n",
    "        self.generator_network = GeneratorNet(mean_xs) \n",
    "        self.eta = Variable(torch.FloatTensor([hparams['eta']]), requires_grad=False)\n",
    "        \n",
    "    def multiply_by_eta(self, grads):\n",
    "        res = []\n",
    "        for g in grads:\n",
    "            res.append(g*self.eta)\n",
    "        return res\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        All of the passes through the Q- and P-networks are here\n",
    "        \"\"\"\n",
    "        ###################################\n",
    "        # REINFORCE step (compute ELBO, etc.)\n",
    "        ###################################\n",
    "        # hardELBO is the non-differentiable learning signal, l(x,b)\n",
    "        #\n",
    "        # reinforce_model_grad is the joint distribution of interest p(x,b,\\theta), \n",
    "        #   and the gradient of l(x,b) wrt the P-network parameters is grad E[logP + logPPrior]  \n",
    "        #   = grad E[reinforce_model_grad]\n",
    "        # \n",
    "        # See https://github.com/tensorflow/models/blob/master/research/rebar/rebar.py#L716\n",
    "        logQHard, hardSamples = self.recognition_network(x)\n",
    "        hardELBO, reinforce_model_grad = self.generator_network(x, hardSamples, logQHard)\n",
    "        logQHard = torch.sum(torch.stack(logQHard), 0)\n",
    "        \n",
    "        ###################################\n",
    "        # compute Gumbel control variate\n",
    "        ###################################\n",
    "        # See https://github.com/tensorflow/models/blob/master/research/rebar/rebar.py#L659\n",
    "        logQ, softSamples = self.recognition_network(x, sampler_=functools.partial(\n",
    "            random_sample_soft, temperature=self._temperature))\n",
    "        softELBO, _ = self.generator_network(x, softSamples, logQ)\n",
    "        # compute softELBO_v (same value as softELBO, different grads) :- zsquiggle = g(v, b, \\theta)\n",
    "        # NOTE: !!! Because of the common random numbers (u_to_v), z is distributed as z|b. \n",
    "        # So the reparameterization for p(z|b) is just g(v,b,\\theta) == g(v,\\theta) == log(\\theta/1-\\theta) + log(v/1-v)\n",
    "        # This is why random_sample_soft_v() just calls random_sample_soft(). I'm 95% sure this is correct...        \n",
    "        logQ_v, softSamples_v = self.recognition_network(x, sampler_=functools.partial(\n",
    "            random_sample_soft_v, temperature=self._temperature))\n",
    "        # should be the same value as softELBO but different grads\n",
    "        softELBO_v, _ = self.generator_network(x, softSamples_v, logQ_v)\n",
    "        gumbel_cv_learning_signal = softELBO_v.detach()\n",
    "        gumbel_cv = gumbel_cv_learning_signal * logQHard - softELBO + softELBO_v\n",
    "        \n",
    "        return {\n",
    "            'logQHard': logQHard,\n",
    "            'hardELBO': hardELBO,\n",
    "            'reinforce_model_grad': reinforce_model_grad,\n",
    "            'gumbel_cv': gumbel_cv\n",
    "        }   \n",
    "\n",
    "class Baseline(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Baseline, self).__init__()\n",
    "        # For centering the learning signal, from the NVIL paper (2.3.1) https://arxiv.org/pdf/1402.0030.pdf\n",
    "        # Input dependent baseline that is trained to minimize the MSE with the learning signal\n",
    "        self.out = nn.Sequential(\n",
    "           nn.Linear(hparams['n_input'], 100),\n",
    "           nn.Tanh(),\n",
    "           nn.Linear(100, 1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random seed\n",
    "random_seed = 1337\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "# Load MNIST dataset\n",
    "train_xs, val_xs, test_xs = datasets.load_data(hparams)\n",
    "# create Dataloader\n",
    "train_dataloader = DataLoader(train_xs, shuffle=True, batch_size=hparams['batch_size'], drop_last=True, num_workers=0)\n",
    "# mean centering on training data\n",
    "mean_xs = Variable(torch.from_numpy(np.mean(train_xs, axis=0)).float(), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbn = SBNRebar(mean_xs)\n",
    "baseline = Baseline()\n",
    "baseline_loss = nn.MSELoss()\n",
    "sbn_opt = optim.Adam(sbn.parameters(), lr=hparams['learning_rate'])\n",
    "baseline_opt = optim.Adam(baseline.parameters(), lr=hparams['learning_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main training loop, where we compute REBAR gradients and update model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                         | 0/2083 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0.0, training objective (ELBO): -267.7251281738281, logGradVar: -8.174643516540527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                 | 2/2083 [00:00<01:53, 18.30it/s]\n",
      "  0%|▏                                                                                | 5/2083 [00:00<01:45, 19.71it/s]\n",
      "  0%|▎                                                                                | 8/2083 [00:00<01:42, 20.20it/s]\n",
      "  1%|▍                                                                               | 11/2083 [00:00<01:41, 20.32it/s]\n",
      "  1%|▍                                                                               | 13/2083 [00:00<01:43, 20.04it/s]\n",
      "  1%|▌                                                                               | 15/2083 [00:00<01:43, 19.95it/s]\n",
      "  1%|▋                                                                               | 17/2083 [00:00<01:43, 19.90it/s]\n",
      "  1%|▋                                                                               | 19/2083 [00:00<01:44, 19.84it/s]\n",
      "  1%|▊                                                                               | 21/2083 [00:01<01:44, 19.66it/s]\n",
      "  1%|▉                                                                               | 23/2083 [00:01<01:44, 19.67it/s]\n",
      "  1%|▉                                                                               | 25/2083 [00:01<01:44, 19.60it/s]\n",
      "  1%|█                                                                               | 27/2083 [00:01<01:45, 19.55it/s]\n",
      "  1%|█                                                                               | 29/2083 [00:01<01:45, 19.48it/s]\n",
      "  1%|█▏                                                                              | 31/2083 [00:01<01:45, 19.49it/s]\n",
      "  2%|█▎                                                                              | 33/2083 [00:01<01:45, 19.50it/s]\n",
      "  2%|█▎                                                                              | 35/2083 [00:01<01:44, 19.51it/s]\n",
      "  2%|█▍                                                                              | 37/2083 [00:01<01:45, 19.45it/s]\n",
      "  2%|█▍                                                                              | 39/2083 [00:02<01:44, 19.47it/s]\n",
      "  2%|█▌                                                                              | 41/2083 [00:02<01:44, 19.48it/s]\n",
      "  2%|█▋                                                                              | 43/2083 [00:02<01:44, 19.48it/s]\n",
      "  2%|█▋                                                                              | 45/2083 [00:02<01:45, 19.39it/s]\n",
      "  2%|█▊                                                                              | 47/2083 [00:02<01:45, 19.34it/s]\n",
      "  2%|█▉                                                                              | 49/2083 [00:02<01:45, 19.33it/s]\n",
      "  2%|█▉                                                                              | 51/2083 [00:02<01:45, 19.35it/s]\n",
      "  3%|██                                                                              | 53/2083 [00:02<01:44, 19.36it/s]\n",
      "  3%|██▏                                                                             | 56/2083 [00:02<01:44, 19.43it/s]\n",
      "  3%|██▎                                                                             | 59/2083 [00:03<01:43, 19.48it/s]\n",
      "  3%|██▍                                                                             | 62/2083 [00:03<01:43, 19.54it/s]\n",
      "  3%|██▍                                                                             | 65/2083 [00:03<01:43, 19.57it/s]\n",
      "  3%|██▌                                                                             | 68/2083 [00:03<01:42, 19.62it/s]\n",
      "  3%|██▋                                                                             | 71/2083 [00:03<01:42, 19.66it/s]\n",
      "  4%|██▊                                                                             | 74/2083 [00:03<01:41, 19.71it/s]\n",
      "  4%|██▉                                                                             | 77/2083 [00:03<01:41, 19.72it/s]\n",
      "  4%|███                                                                             | 80/2083 [00:04<01:41, 19.74it/s]\n",
      "  4%|███▏                                                                            | 83/2083 [00:04<01:41, 19.77it/s]\n",
      "  4%|███▎                                                                            | 86/2083 [00:04<01:41, 19.77it/s]\n",
      "  4%|███▍                                                                            | 89/2083 [00:04<01:40, 19.80it/s]\n",
      "  4%|███▌                                                                            | 92/2083 [00:04<01:40, 19.82it/s]\n",
      "  5%|███▋                                                                            | 95/2083 [00:04<01:40, 19.80it/s]\n",
      "  5%|███▊                                                                            | 98/2083 [00:04<01:40, 19.79it/s]\n",
      "  5%|███▊                                                                           | 100/2083 [00:05<01:40, 19.78it/s]\n",
      "  5%|███▊                                                                           | 102/2083 [00:05<01:40, 19.78it/s]\n",
      "  5%|███▉                                                                           | 104/2083 [00:05<01:40, 19.74it/s]\n",
      "  5%|████                                                                           | 106/2083 [00:05<01:40, 19.72it/s]\n",
      "  5%|████                                                                           | 108/2083 [00:05<01:40, 19.72it/s]\n",
      "  5%|████▏                                                                          | 111/2083 [00:05<01:39, 19.75it/s]\n",
      "  5%|████▎                                                                          | 113/2083 [00:05<01:39, 19.75it/s]\n",
      "  6%|████▎                                                                          | 115/2083 [00:05<01:39, 19.72it/s]\n",
      "  6%|████▍                                                                          | 117/2083 [00:05<01:39, 19.68it/s]\n",
      "  6%|████▌                                                                          | 120/2083 [00:06<01:39, 19.69it/s]\n",
      "  6%|████▋                                                                          | 123/2083 [00:06<01:39, 19.70it/s]\n",
      "  6%|████▋                                                                          | 125/2083 [00:06<01:39, 19.70it/s]\n",
      "  6%|████▊                                                                          | 127/2083 [00:06<01:39, 19.70it/s]\n",
      "  6%|████▉                                                                          | 129/2083 [00:06<01:39, 19.69it/s]\n",
      "  6%|████▉                                                                          | 131/2083 [00:06<01:39, 19.67it/s]\n",
      "  6%|█████                                                                          | 133/2083 [00:06<01:39, 19.67it/s]\n",
      "  7%|█████▏                                                                         | 136/2083 [00:06<01:38, 19.69it/s]\n",
      "  7%|█████▎                                                                         | 139/2083 [00:07<01:38, 19.71it/s]\n",
      "  7%|█████▎                                                                         | 141/2083 [00:07<01:38, 19.71it/s]\n",
      "  7%|█████▍                                                                         | 143/2083 [00:07<01:38, 19.71it/s]\n",
      "  7%|█████▍                                                                         | 145/2083 [00:07<01:38, 19.71it/s]\n",
      "  7%|█████▌                                                                         | 147/2083 [00:07<01:38, 19.71it/s]\n",
      "Exception in thread Thread-19:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\pemami\\Continuum\\anaconda3\\lib\\threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\pemami\\Continuum\\anaconda3\\lib\\site-packages\\tqdm\\_tqdm.py\", line 148, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"C:\\Users\\pemami\\Continuum\\anaconda3\\lib\\_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n",
      " 24%|██████████████████▉                                                            | 500/2083 [00:24<01:18, 20.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 500.0, training objective (ELBO): -275.3807373046875, logGradVar: -0.5940424799919128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|█████████████████████████████████████▉                                         | 999/2083 [00:49<00:53, 20.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1000.0, training objective (ELBO): -257.50360107421875, logGradVar: -0.4952937662601471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|████████████████████████████████████████████████████████▏                     | 1499/2083 [01:13<00:28, 20.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1500.0, training objective (ELBO): -276.4314880371094, logGradVar: -0.5120500326156616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████████████████████████████████████████████████████████                   | 1576/2083 [01:17<00:24, 20.30it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-81113e5eff50>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0msbn_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msbn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mbaseline_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbaseline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-29-eeb43d3d3ead>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[1;31m# See https://github.com/tensorflow/models/blob/master/research/rebar/rebar.py#L659\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         logQ, softSamples = self.recognition_network(x, sampler_=functools.partial(\n\u001b[1;32m---> 39\u001b[1;33m             random_sample_soft, temperature=self._temperature))\n\u001b[0m\u001b[0;32m     40\u001b[0m         \u001b[0msoftELBO\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerator_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msoftSamples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogQ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;31m# compute softELBO_v (same value as softELBO, different grads) :- zsquiggle = g(v, b, \\theta)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-28-1643537b835d>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, sampler_)\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0msamples\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'activation'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m             \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m             \u001b[1;31m# expect sampler to return a dictionary with key 'activation'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             samples[i], self.uniform_samples_v = sampler(logits, self.uniform_samples[i],\n",
      "\u001b[1;32m~\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-a471c281b4b4>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m    833\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    834\u001b[0m         \u001b[1;31m# fused op is marginally faster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 835\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    836\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    837\u001b[0m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Exponential Moving Average for log variance calculation\n",
    "ema_first_moment = 0.\n",
    "ema_second_moment = 0.\n",
    "beta = hparams['ema_beta']\n",
    "log_every = hparams['log_every']\n",
    "n = hparams['train_steps']\n",
    "\n",
    "step = 0.\n",
    "while step < n:\n",
    "    for x in tqdm(train_dataloader):\n",
    "        x = Variable(x, requires_grad=False)\n",
    "        sbn_outs = sbn.forward(x)\n",
    "        baseline_out = baseline.forward(x)\n",
    "\n",
    "        nvil_gradient = (sbn_outs['hardELBO'].detach() - baseline_out) * sbn_outs['logQHard'] \\\n",
    "            + sbn_outs['reinforce_model_grad']\n",
    "\n",
    "        f_grads = grad(-nvil_gradient.mean(), sbn.parameters(), retain_graph=True)\n",
    "        gumbel_grads = grad(sbn_outs['gumbel_cv'].mean(), sbn.parameters())\n",
    "        h_grads = sbn.multiply_by_eta(gumbel_grads)\n",
    "        total_grads = [(g_a + g_b) for (g_a, g_b) in zip(f_grads, h_grads)]\n",
    "\n",
    "        # training objective\n",
    "        lhat = sbn_outs['hardELBO'].mean().detach()\n",
    "\n",
    "        # baseline loss\n",
    "        baseline_y = baseline_loss(baseline_out, sbn_outs['hardELBO'].detach())\n",
    "\n",
    "        # variance summaries\n",
    "        first_moment = U.vectorize(total_grads, skip_none=True)\n",
    "        second_moment = first_moment ** 2\n",
    "        ema_first_moment = (beta * ema_first_moment) + (1 - beta) * first_moment\n",
    "        ema_second_moment = (beta * ema_second_moment) + (1 - beta) * second_moment\n",
    "        log_grad_variance = torch.log((ema_second_moment - (ema_first_moment ** 2)).mean())\n",
    "\n",
    "        sbn_opt.zero_grad()\n",
    "        baseline_opt.zero_grad()\n",
    "\n",
    "        # set model grads with REBAR gradients\n",
    "        for (g, p) in zip(total_grads, sbn.parameters()):\n",
    "            p.grad = g\n",
    "\n",
    "        sbn_opt.step()\n",
    "        # update baseline\n",
    "        baseline_y.backward()\n",
    "        baseline_opt.step()\n",
    "\n",
    "        if step % log_every == 0:        \n",
    "            print('step: {}, training objective (ELBO): {}, logGradVar: {}'.format(step, lhat.data[0], log_grad_variance.data[0]))\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
