{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of REBAR (https://arxiv.org/abs/1703.07370), a low-variance, unbiased gradient estimator for discrete latent variable models. This notebook is focused on the generative modeling experiments on the MNIST and Omniglot datasets from Section 5.2.1.\n",
    "\n",
    "The problem being solved is $\\text{max} \\hspace{5px} \\mathbb{E} [f(b, \\theta) | p(b) ]$, $b$ ~ Bernoulli($\\theta$).\n",
    "\n",
    "For generative modeling, the objective is to maximize a single-sample variational lower bound on the log-likelihood. There are two networks, one to model $q(b|x,\\theta)$ and one to model $p(x,b|\\theta)$. The former is the variational distribution and the latter is the joint probability distribution over the data and latent stochastic variables $b$.\n",
    "\n",
    "The **ELBO**, or evidence lower bound which we seek to maximize, is: \n",
    "\n",
    "$$\n",
    "\\log p(x \\vert \\theta) \\geq \\mathbb{E}_{q(b \\vert x,\\theta)} [ \\log p(x,b\\vert\\theta) - \\log q(b \\vert x,\\theta)]\n",
    "$$\n",
    "\n",
    "In practice, the Q-network has its own set of parameters $\\phi$ and the generator network $P$ has its own parameters $\\theta$.\n",
    "\n",
    "I'll refer to the learning signal $\\log p(x,b\\vert\\theta) - \\log q(b \\vert x,\\theta)$ as $l(x,b)$ for shorthand.\n",
    "\n",
    "The following is an implementation of a Sigmoid Belief Network (SBN) with REBAR gradient updates. I tried to follow the [author's TensorFlow implementation](https://github.com/tensorflow/models/blob/master/research/rebar/rebar.py) closely for correctness; there are a lot of computational statistics stuff going on that need to be implemented carefully.\n",
    "\n",
    "For an in-depth treatment on SBNs, see [this paper](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.63.1777&rep=rep1&type=pdf) by R. Neal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're just going to focus on the nonlinear SBN REBAR model.\n",
    "The model is pretty complex, so I'll implement them as separate modules and try to explain them\n",
    "one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import grad\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import rebar.datasets as datasets\n",
    "import rebar.util as U\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some global parameters we'll need later\n",
    "hparams = {\n",
    "    'model': 'SBNGumbel',\n",
    "    'learning_rate':3e-4,\n",
    "    'n_hidden':200,\n",
    "    'n_input':784,\n",
    "    'temperature':0.5,\n",
    "    'eta':0.1,\n",
    "    'batch_size':24,\n",
    "    'task':'sbn',\n",
    "    'n_layers': 2,\n",
    "    'dynamic_b': False,\n",
    "    'ema_beta': 0.999,\n",
    "    'train_steps': 200000,\n",
    "    'log_every': 100\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define samplers for producing the \"hard\" and \"soft\" reparameterized samples needed for computing the REBAR gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_sample(log_alpha, u, layer, uniform_samples_v, temperature=None):\n",
    "    \"\"\"Returns sampled random variables parameterized by log_alpha.\"\"\"\n",
    "    # Generate tied randomness for later\n",
    "    if layer not in uniform_samples_v:\n",
    "        uniform_samples_v[layer] = u_to_v(log_alpha, u)\n",
    "        \n",
    "    # Sample random variable underlying softmax/argmax\n",
    "    x = log_alpha + U.safe_log_prob(u) - U.safe_log_prob(1 - u)\n",
    "    samples = ((x > 0).float()).detach()\n",
    "\n",
    "    return {\n",
    "        'preactivation': x,\n",
    "        'activation': samples,\n",
    "        'log_param': log_alpha,\n",
    "    }, uniform_samples_v\n",
    "\n",
    "def random_sample_soft(log_alpha, u, layer, uniform_samples_v, temperature=None):\n",
    "    \"\"\"Returns sampled random variables parameterized by log_alpha.\"\"\"\n",
    "\n",
    "    # Sample random variable underlying softmax/argmax\n",
    "    x = log_alpha + U.safe_log_prob(u) - U.safe_log_prob(1 - u)\n",
    "    x = x / temperature.view(-1)\n",
    "    y = F.sigmoid(x)\n",
    "\n",
    "    return {\n",
    "        'preactivation': x,\n",
    "        'activation': y,\n",
    "        'log_param': log_alpha\n",
    "    }, uniform_samples_v\n",
    "\n",
    "def random_sample_soft_v(log_alpha, _, layer, uniform_samples_v, temperature=None):\n",
    "    \"\"\"Returns sampled random variables parameterized by log_alpha.\"\"\"\n",
    "    v = uniform_samples_v[layer]\n",
    "    return random_sample_soft(log_alpha, v, layer, uniform_samples_v, temperature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next bit, for producing common random numbers, is for variance reduction. [The general idea is easy](https://en.wikipedia.org/wiki/Variance_reduction), but what the authors were doing here is a bit more subtle. According to Appendix G.2, they're correlating u and v to reduce the variance of the gradient by first sampling u and then using that to determine v."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " # Random samplers TODO\n",
    "def u_to_v(log_alpha, u, eps = 1e-8):\n",
    "    \"\"\"Convert u to tied randomness in v.\"\"\"\n",
    "    u_prime = F.sigmoid(-log_alpha)  # g(u') = 0\n",
    "    v_1 = (u - u_prime) / torch.clamp(1 - u_prime, eps, 1)\n",
    "    v_1 = torch.clamp(v_1, 0, 1).detach()\n",
    "    v_1 = v_1*(1 - u_prime) + u_prime\n",
    "    v_0 = u / torch.clamp(u_prime, eps, 1)\n",
    "    v_0 = torch.clamp(v_0, 0, 1).detach()\n",
    "    v_0 = v_0 * u_prime\n",
    "    v = u.clone()\n",
    "    v[(u > u_prime).detach()] = v_1\n",
    "    v[(u <= u_prime).detach()] = v_0\n",
    "    # TODO: add pytorch check\n",
    "    #v = tf.check_numerics(v, 'v sampling is not numerically stable.')\n",
    "    v = v + (-v + u).detach()  # v and u are the same up to numerical errors\n",
    "    return Variable(v.data, requires_grad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the deterministic mapping we'll use to construct the stochastic layers of the Q- and P-networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Transformation(nn.Module):\n",
    "    \"\"\"\n",
    "    Deterministic transformation between stochastic layers\n",
    "    \n",
    "        x -> FC -> Tanh -> FC -> Tanh() -> FC -> logQ\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, n_input, n_hidden, n_output):\n",
    "        super(Transformation, self).__init__()\n",
    "        self.h = nn.Sequential(\n",
    "            nn.Linear(n_input, n_hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(n_hidden, n_output))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.h(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RecognitionNet is the variational distribution (Q-network) and the GeneratorNet is the joint distribution of the data and latent variables (P-network). It looks like this for an unrolled 2-layer SBN, where Sample is the stochastic layer of Bernoulli units:\n",
    "\n",
    "// Replace with figure?\n",
    "\n",
    "x -> Transformation(x) -> Sample(x) -> Transformation(x) -> Sample(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RecognitionNet(nn.Module):\n",
    "    \"\"\"\n",
    "    given x values, samples from Q and returns log Q(h|x)\n",
    "    \"\"\"\n",
    "    def __init__(self, mean_xs, sampler):\n",
    "        super(RecognitionNet, self).__init__()\n",
    "        self.mean_xs = mean_xs\n",
    "        self.sampler = sampler\n",
    "        self.transforms = nn.ModuleList([Transformation(hparams['n_input'],\n",
    "                                        hparams['n_hidden'], hparams['n_hidden'])])\n",
    "        if hparams['n_layers'] > 1:\n",
    "            for _ in range(1, hparams['n_layers']):\n",
    "                self.transforms.append(Transformation(hparams['n_hidden'], hparams['n_hidden'],\n",
    "                                                     hparams['n_hidden']))\n",
    "        self.uniform_samples = dict()\n",
    "        self.uniform_samples_v = dict()\n",
    "        # generate randomness\n",
    "        for i in range(hparams['n_layers']):\n",
    "            self.uniform_samples[i] = Variable(\n",
    "                torch.FloatTensor(hparams['batch_size'], hparams['n_hidden']).uniform_(0,1),\n",
    "                requires_grad=False)\n",
    "            \n",
    "    def forward(self, x, sampler_=None):\n",
    "        if sampler_ is not None:\n",
    "            sampler = sampler_\n",
    "        else:\n",
    "            sampler = self.sampler\n",
    "        samples = {}\n",
    "        samples[-1] = {'activation': x}\n",
    "        samples[-1]['activation'] -= mean_xs\n",
    "        samples[-1]['activation'] = (samples[-1]['activation'] + 1)/2.\n",
    "        logQ = []\n",
    "        for i,t in enumerate(self.transforms):\n",
    "            input = 2 * samples[i-1]['activation'] - 1.0\n",
    "            logits = t(input)\n",
    "            # expect sampler to return a dictionary with key 'activation'\n",
    "            samples[i], self.uniform_samples_v = sampler(logits, self.uniform_samples[i],\n",
    "                                                         i, self.uniform_samples_v)\n",
    "            logQ.append(U.binary_log_likelihood(samples[i]['activation'], logits))  \n",
    "        # logQHard, samples\n",
    "        return logQ, samples\n",
    "\n",
    "class GeneratorNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Returns learning signal and function. Reconstructs the input.\n",
    "    \"\"\"\n",
    "    def __init__(self, mean_xs):\n",
    "        super(GeneratorNet, self).__init__()\n",
    "        self.transforms = []\n",
    "        for i in range(hparams['n_layers']):\n",
    "            if i == 0:\n",
    "                n_output = hparams['n_input']\n",
    "            else:\n",
    "                n_output = hparams['n_hidden']\n",
    "            self.transforms.append(Transformation(hparams['n_hidden'],\n",
    "                                                 hparams['n_hidden'], n_output))\n",
    "        self.prior = Variable(torch.zeros(hparams['n_hidden']), requires_grad=False)\n",
    "        self.train_bias = -np.log(1./np.clip(mean_xs.data.numpy(), 0.001, 0.999)-1.).astype(np.float32)\n",
    "        self.train_bias = Variable(torch.from_numpy(self.train_bias).float(), requires_grad=False)\n",
    "        \n",
    "    def forward(self, x, samples, logQ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            samples: dictionary of sampled latent variables\n",
    "            logQ: list of log q(h_i) terms\n",
    "        \"\"\"\n",
    "        sum_logQ = torch.sum(torch.stack(logQ))\n",
    "        logPPrior = U.binary_log_likelihood(samples[hparams['n_layers']-1]['activation'], self.prior)\n",
    "        for i in reversed(range(hparams['n_layers'])):\n",
    "            # Set up the input to the layer\n",
    "            input = 2 * samples[i]['activation'] - 1.0\n",
    "            h = self.transforms[i](input)\n",
    "            if i == 0:\n",
    "                logP = U.binary_log_likelihood(x, h + self.train_bias)\n",
    "            else:\n",
    "                logPPrior += U.binary_log_likelihood(samples[i-1]['activation'], h)\n",
    "        # Note that logP(x,b) = logP(b|x) + logP(x)\n",
    "        # reinforce_learning_signal (l(x,b)), reinforce_model_grad\n",
    "        return logP + logPPrior - sum_logQ, logP + logPPrior         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can put these modules together inside the SBNRebar module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SBNRebar(nn.Module):\n",
    "    def __init__(self, mean_xs):\n",
    "        super(SBNRebar, self).__init__()\n",
    "        self.mean_xs = mean_xs   \n",
    "        self._temperature = Variable(torch.FloatTensor([hparams['temperature']]), requires_grad=False)\n",
    "        self.recognition_network = RecognitionNet(mean_xs, random_sample)\n",
    "        self.generator_network = GeneratorNet(mean_xs) \n",
    "        self.eta = Variable(torch.FloatTensor([hparams['eta']]), requires_grad=False)\n",
    "        \n",
    "    def multiply_by_eta(self, grads):\n",
    "        res = []\n",
    "        for g in grads:\n",
    "            res.append(g*eta)\n",
    "        return res\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        All of the passes through the Q- and P-networks are here\n",
    "        \"\"\"\n",
    "        # hardELBO is the non-differentiable learning signal, l(x,b)\n",
    "        #\n",
    "        # reinforce_model_grad is the joint distribution of interest p(x,b,\\theta), \n",
    "        #   and the gradient of l(x,b) wrt the P-network parameters is grad E[logP + logPPrior]  \n",
    "        #   = grad E[reinforce_model_grad]\n",
    "        logQHard, hardSamples = self.recognition_network(x)\n",
    "        hardELBO, reinforce_model_grad = self.generator_network(x, hardSamples, logQHard)\n",
    "        logQHard = torch.stack(logQHard).sum()\n",
    "        \n",
    "        # compute Gumbel CV\n",
    "        logQ, softSamples = self.recognition_network(x, sampler_=functools.partial(\n",
    "            random_sample_soft, temperature=self._temperature))\n",
    "        softELBO, _ = self.generator_network(x, softSamples, logQ)\n",
    "        # compute softELBO_v (same value as softELBO, different grads) :- zsquiggle = g(v, b, \\theta)\n",
    "        # NOTE: !!!Super Tricky!!! Because of the common random numbers (u_to_v), z is distributed as z|b. \n",
    "        # So the reparameterization for p(z|b) is just g(v,b,\\theta) == g(v,\\theta) == log(\\theta/1-\\theta) + log(v/1-v)\n",
    "        # This is why random_sample_soft_v() just calls random_sample_soft()        \n",
    "        logQ_v, softSamples_v = self.recognition_network(x, sampler_=functools.partial(\n",
    "            random_sample_soft_v, temperature=self._temperature))\n",
    "        softELBO_v, _ = self.generator_network(x, softSamples_v, logQ_v)\n",
    "        \n",
    "        gumbel_cv_learning_signal = softELBO_v.detach()\n",
    "        gumbel_cv = gumbel_cv_learning_signal * logQHard - softELBO + softELBO_v\n",
    "        \n",
    "        return {\n",
    "            'logQHard': logQHard,\n",
    "            'hardELBO': hardELBO,\n",
    "            'reinforce_model_grad': reinforce_model_grad,\n",
    "            'gumbel_cv': gumbel_cv\n",
    "        }   \n",
    "\n",
    "class Baseline(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Baseline, self).__init__()\n",
    "        # For centering the learning signal, from the NVIL paper (2.3.1) https://arxiv.org/pdf/1402.0030.pdf\n",
    "        # Input dependent baseline that is trained to minimize the MSE with the learning signal\n",
    "        self.out = nn.Sequential(\n",
    "           nn.Linear(hparams['n_input'], 100),\n",
    "           nn.Tanh(),\n",
    "           nn.Linear(100, 1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Random seed\n",
    "random_seed = 1337\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "# Load MNIST dataset\n",
    "train_xs, val_xs, test_xs = datasets.load_data(hparams)\n",
    "train_dataloader = DataLoader(train_xs, shuffle=True, batch_size=hparams['batch_size'], num_workers=0)\n",
    "# mean centering on training data\n",
    "mean_xs = Variable(torch.from_numpy(np.mean(train_xs, axis=0)).float(), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sbn = SBNRebar(mean_xs)\n",
    "baseline = Baseline()\n",
    "baseline_loss = nn.MSELoss()\n",
    "sbn_opt = optim.Adam(sbn.parameters(), lr=hparams['learning_rate'])\n",
    "baseline_opt = optim.Adam(baseline.parameters(), lr=hparams['learning_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main training loop, where we compute REBAR gradients and update model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-40792aa55926>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mf_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnvil_gradient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msbn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mgumbel_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msbn_outs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gumbel_cv'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msbn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mh_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msbn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply_by_eta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgumbel_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mtotal_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_a\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mg_b\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mg_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_b\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_grads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/pemami/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[1;32m    156\u001b[0m     return Variable._execution_engine.run_backward(\n\u001b[1;32m    157\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         inputs, only_inputs, allow_unused)\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_autograd_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation"
     ]
    }
   ],
   "source": [
    "# Exponential Moving Average for log variance calculation\n",
    "ema_first_moment = 0.\n",
    "ema_second_moment = 0.\n",
    "beta = hparams['ema_beta']\n",
    "log_every = hparams['log_every']\n",
    "n = hparams['train_steps']\n",
    "\n",
    "# create Dataloader\n",
    "for step, x in tqdm(enumerate(train_dataloader)):\n",
    "    x = Variable(x, requires_grad=False)\n",
    "    sbn_outs = sbn.forward(x)\n",
    "    baseline_out = baseline.forward(x)\n",
    "    \n",
    "    nvil_gradient = (sbn_outs['hardELBO'].detach() - baseline_out) * sbn_outs['logQHard'] \\\n",
    "        + sbn_outs['reinforce_model_grad']\n",
    "        \n",
    "    f_grads = grad(-nvil_gradient.mean(), sbn.parameters())\n",
    "    gumbel_grads = grad(sbn_outs['gumbel_cv'].mean(), sbn.parameters())\n",
    "    h_grads = sbn.multiply_by_eta(gumbel_grads)\n",
    "    total_grads = [(g_a + g_b) for (g_a, g_b) in zip(f_grads, h_grads)]\n",
    "     \n",
    "    # training objective\n",
    "    lhat = sbn_outs['hardELBO'].mean().detach()\n",
    "    \n",
    "    # baseline loss\n",
    "    baseline_y = baseline_loss(baseline_out, sbn_outs['hardELBO'].detach())\n",
    "    \n",
    "    # variance summaries\n",
    "    first_moment = U.vectorize(total_grads, skip_none=True)\n",
    "    second_moment = first_moment ** 2\n",
    "    ema_first_moment = (beta * ema_first_moment) + (1 - beta) * first_moment\n",
    "    ema_second_moment = (beta * ema_second_moment) + (1 - beta) * second_moment\n",
    "    log_grad_variance = torch.log(ema_second_moment - (ema_first_moment ** 2).mean())\n",
    "    \n",
    "    sbn_opt.zero_grad()\n",
    "    baseline_opt.zero_grad()\n",
    "    \n",
    "    # set model grads with REBAR gradients\n",
    "    for (g, p) in zip(total_grads, sbn.parameters()):\n",
    "        p.grad = g\n",
    "    \n",
    "    sbn_opt.step()\n",
    "    # update baseline\n",
    "    baseline_y.backward()\n",
    "    baseline_opt.step()\n",
    "    \n",
    "    if step % log_every == 0:        \n",
    "        print('step: {}, training objective (ELBO): {}, logGradVar: {}'.format(step, lhat.data[0], log_grad_variance.data[0]))\n",
    "        if step > n:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
