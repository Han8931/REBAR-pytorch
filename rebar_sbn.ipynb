{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of REBAR (https://arxiv.org/abs/1703.07370), a low-variance, unbiased gradient estimator for discrete latent variable models. This notebook is focused on the generative modeling experiments on the MNIST and Omniglot datasets from Section 5.2.1.\n",
    "\n",
    "The problem being solved is $\\text{max} \\hspace{5px} \\mathbb{E} [f(b, \\theta) | p(b) ]$, $b$ ~ Bernoulli($\\theta$).\n",
    "\n",
    "For generative modeling, the objective is to maximize a single-sample variational lower bound on the log-likelihood. There are two networks, one to model $q(b|x,\\theta)$ and one to model $p(x,b|\\theta)$. The former is the variational distribution and the latter is the joint probability distribution over the data and latent stochastic variables $b$.\n",
    "\n",
    "The **ELBO**, or evidence lower bound which we seek to maximize, is: \n",
    "\n",
    "$$\n",
    "\\log p(x \\vert \\theta) \\geq \\mathbb{E}_{q(b \\vert x,\\theta)} [ \\log p(x,b\\vert\\theta) - \\log q(b \\vert x,\\theta)]\n",
    "$$\n",
    "\n",
    "In practice, the Q-network has its own set of parameters $\\phi$ and the generator network $P$ has its own parameters $\\theta$.\n",
    "\n",
    "I'll refer to the learning signal $\\log p(x,b\\vert\\theta) - \\log q(b \\vert x,\\theta)$ as $l(x,b)$ for shorthand.\n",
    "\n",
    "The following is an implementation of a Sigmoid Belief Network (SBN) with REBAR gradient updates. I tried to follow the [author's TensorFlow implementation](https://github.com/tensorflow/models/blob/master/research/rebar/rebar.py) closely for correctness; there are a lot of computational statistics stuff going on that need to be implemented carefully.\n",
    "\n",
    "For an in-depth treatment on SBNs, see [this paper](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.63.1777&rep=rep1&type=pdf) by R. Neal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're just going to focus on the nonlinear SBN REBAR model.\n",
    "The model is pretty complex, so I'll implement them as separate modules and try to explain them\n",
    "one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataloader\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import grad\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import rebar.datasets as datasets\n",
    "\n",
    "import rebar.util as U\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some global parameters we'll need later\n",
    "hparams = {\n",
    "    'model': 'SBNGumbel',\n",
    "    'learning_rate':3e-4,\n",
    "    'n_hidden':200,\n",
    "    'n_input':784,\n",
    "    'temperature':0.5,\n",
    "    'batch_size':24,\n",
    "    'task':'sbn',\n",
    "    'n_layers': 2,\n",
    "    'dynamic_b': False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define samplers for producing the \"hard\" and \"soft\" reparameterized samples needed for computing the REBAR gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sample(log_alpha, u, layer, uniform_samples_v, _):\n",
    "    \"\"\"Returns sampled random variables parameterized by log_alpha.\"\"\"\n",
    "    # Generate tied randomness for later\n",
    "    if layer not in uniform_samples_v:\n",
    "        uniform_samples_v[layer] = u_to_v(log_alpha, u)\n",
    "        \n",
    "    # Sample random variable underlying softmax/argmax\n",
    "    x = log_alpha + U.safe_log_prob(u) - U.safe_log_prob(1 - u)\n",
    "    samples = ((x > 0).float()).detach()\n",
    "\n",
    "    return {\n",
    "        'preactivation': x,\n",
    "        'activation': samples,\n",
    "        'log_param': log_alpha,\n",
    "    }, uniform_samples_v\n",
    "\n",
    "def random_sample_soft(log_alpha, u, layer, uniform_samples_v, temperature=None):\n",
    "    \"\"\"Returns sampled random variables parameterized by log_alpha.\"\"\"\n",
    "\n",
    "    # Sample random variable underlying softmax/argmax\n",
    "    x = log_alpha + U.safe_log_prob(u) - U.safe_log_prob(1 - u)\n",
    "    x /= temperature.view(-1)\n",
    "    y = F.sigmoid(x)\n",
    "\n",
    "    return {\n",
    "        'preactivation': x,\n",
    "        'activation': y,\n",
    "        'log_param': log_alpha\n",
    "    }, uniform_samples_v\n",
    "\n",
    "def random_sample_soft_v(log_alpha, _, layer, uniform_samples_v, temperature=None):\n",
    "    \"\"\"Returns sampled random variables parameterized by log_alpha.\"\"\"\n",
    "    v = uniform_samples[layer]\n",
    "    return random_sample_soft(log_alpha, v, layer, uniform_samples_v, temperature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next bit, for producing common random numbers, is for variance reduction. [The general idea is easy](https://en.wikipedia.org/wiki/Variance_reduction), but what the authors were doing here is a bit more subtle. According to Appendix G.2, they're correlating u and v to reduce the variance of the gradient by first sampling u and then using that to determine v."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Random samplers TODO\n",
    "def u_to_v(log_alpha, u, eps = 1e-8):\n",
    "    \"\"\"Convert u to tied randomness in v.\"\"\"\n",
    "    u_prime = F.sigmoid(-log_alpha)  # g(u') = 0\n",
    "\n",
    "    v_1 = (u - u_prime) / torch.clamp(1 - u_prime, eps, 1)\n",
    "    v_1 = torch.clamp(v_1, 0, 1).detach()\n",
    "    v_1 = v_1*(1 - u_prime) + u_prime\n",
    "    v_0 = u / torch.clamp(u_prime, eps, 1)\n",
    "    v_0 = torch.clamp(v_0, 0, 1).detach()\n",
    "    v_0 = v_0 * u_prime\n",
    "    v = v_1 if(u > u_prime) else v_0\n",
    "    # TODO: add pytorch check\n",
    "    #v = tf.check_numerics(v, 'v sampling is not numerically stable.')\n",
    "    v = v + (-v + u).detach()  # v and u are the same up to numerical errors\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the deterministic mapping we'll use to construct the stochastic layers of the Q- and P-networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformation(nn.Module):\n",
    "    \"\"\"\n",
    "    Deterministic transformation between stochastic layers\n",
    "    \n",
    "        x -> FC -> Tanh -> FC -> Tanh() -> FC -> logQ\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, n_input, n_hidden, n_output):\n",
    "        super(Transformation, self).__init__()\n",
    "        self.h = nn.Sequential(\n",
    "            nn.Linear(n_input, n_hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(n_hidden, n_output))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.h(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RecognitionNet is the variational distribution (Q-network) and the GeneratorNet is the joint distribution of the data and latent variables (P-network). It looks like this for an unrolled 2-layer SBN, where Sample is the stochastic layer of Bernoulli units:\n",
    "\n",
    "// Replace with figure?\n",
    "\n",
    "x -> Transformation(x) -> Sample(x) -> Transformation(x) -> Sample(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecognitionNet(nn.Module):\n",
    "    \"\"\"\n",
    "    given x values, samples from Q and returns log Q(h|x)\n",
    "    \"\"\"\n",
    "    def __init__(self, mean_xs, uniform_samples, sampler):\n",
    "        super(RecognitionNet, self).__init__()\n",
    "        self.mean_xs = mean_xs\n",
    "        self.uniform_samples = uniform_samples\n",
    "        self.sampler = sampler\n",
    "        self.transforms = nn.ModuleList([Transformation(hparams.n_input,\n",
    "                                        hparams.n_hidden, hparams.n_hidden)\n",
    "                                         for _ in range(hparams.n_layers)])\n",
    "        self.uniform_samples = dict()\n",
    "        self.uniform_samples_v = dict()\n",
    "        # generate randomness\n",
    "        for i in range(self.hparams.n_layers):\n",
    "            self.uniform_samples[i] = Variable(\n",
    "                torch.FloatTensor([self.batch_size, self.hparams.n_hidden]).uniform_(0,1), requires_grad=False)\n",
    "            \n",
    "    def forward(self, x, sampler_=None):\n",
    "        if sampler_ is not None:\n",
    "            sampler = sampler_\n",
    "        else:\n",
    "            sampler = self.sampler\n",
    "        samples = {}\n",
    "        samples[-1] = {'activation': x}\n",
    "        samples[-1] -= mean_xs\n",
    "        samples[-1] = (samples[-1] + 1)/2.\n",
    "        logQ = []\n",
    "        for i,t in enumerate(self.transforms):\n",
    "            input = 2 * samples[i-1]['activation'] - 1.0\n",
    "            logits = t(input)\n",
    "            # expect sampler to return a dictionary with key 'activation'\n",
    "            samples[i], self.uniform_samples_v = sampler(logits, self.uniform_samples[i],\n",
    "                                                         i, self.uniform_samples_v)\n",
    "            logQ.append(U.binary_log_likelihood(samples[-1], logits))  \n",
    "        # logQHard, samples\n",
    "        return logQ, samples\n",
    "\n",
    "class GeneratorNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Returns learning signal and function. Reconstructs the input.\n",
    "    \"\"\"\n",
    "    def __init__(self, mean_xs):\n",
    "        self.transforms = nn.ModuleList()\n",
    "        for i in range(hparams.n_layers):\n",
    "            if i == 0:\n",
    "                n_output = hparams.n_input\n",
    "            else:\n",
    "                n_output = hparams.n_hidden\n",
    "            self.transforms.append(Transformation(hparams.n_input,\n",
    "                                                 hparams.n_hidden, n_output))\n",
    "        self.prior = Variable(torch.zeros(hparams.n_hidden), requires_grad=False)\n",
    "        self.train_bias= -np.log(1./np.clip(mean_xs, 0.001, 0.999)-1.).astype(np.float32)\n",
    "        \n",
    "    def forward(self, x, samples, logQ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            samples: dictionary of sampled latent variables\n",
    "            logQ: list of log q(h_i) terms\n",
    "        \"\"\"\n",
    "        logPPrior = U.binary_log_likelihood(samples[hparams.n_layers-1], self.prior)\n",
    "        for i in reversed(range(hparams.n_layers)):\n",
    "            # Set up the input to the layer\n",
    "            input = 2 * samples[i]['activation'] - 1.0\n",
    "            h = self.transforms[i](input)\n",
    "            if i == 0:\n",
    "                logP = U.binary_log_likelihood(x, h + self.train_bias)\n",
    "            else:\n",
    "                logPPrior += U.binary_log_likelihood(samples[i-1], h)\n",
    "        # Note that logP(x,b) = logP(b|x) + logP(x)\n",
    "        # reinforce_learning_signal (l(x,b)), reinforce_model_grad\n",
    "        return logP + logPPrior - torch.sum(logQ), logP + logPPrior         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can put these modules together inside the SBNRebar module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SBNRebar(nn.Module):\n",
    "    def __init__(self, hparams, mean_xs):\n",
    "        super(SBN, self).__init__()\n",
    "        self.mean_xs     \n",
    "        self._temperature = Variable(torch.FloatTensor(hparams.temperature), requires_grad=False)\n",
    "        self.recognition_network = RecognitionNet(mean_xs, random_sample)\n",
    "        self.generator_network = GeneratorNet(mean_xs) \n",
    "                  \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        All of the passes through the Q- and P-networks are here\n",
    "        \"\"\"\n",
    "        # hardELBO is the non-differentiable learning signal, l(x,b)\n",
    "        #\n",
    "        # reinforce_model_grad is the joint distribution of interest p(x,b,\\theta), \n",
    "        #   and the gradient of l(x,b) wrt the P-network parameters is grad E[logP + logPPrior]  \n",
    "        #   = grad E[reinforce_model_grad]\n",
    "        logQHard, hardSamples = self.recognition_network(x)\n",
    "        hardELBO, reinforce_model_grad = self.generator_network(x, hardSamples, logQHard)\n",
    "        logQHard = torch.sum(logQHard)\n",
    "        \n",
    "        # compute Gumbel control variate\n",
    "        logQ, softSamples = self.recognition_network(sampler=functools.partial(\n",
    "            random_sample_soft, temperature=self._temperature))\n",
    "        softELBO, _ = self.generator_network(x, softSamples, logQ)\n",
    "        logQ = torch.sum(logQ)\n",
    "        \n",
    "        # compute softELBO_v (same value as softELBO, different grads) :- zsquiggle = g(v, b, \\theta)\n",
    "        # NOTE: !!!Super Tricky!!! Because of the common random numbers (u_to_v), z is distributed as z|b. \n",
    "        # So the reparameterization for p(z|b) is just g(v,b,\\theta) == g(v,\\theta) == log(\\theta/1-\\theta) + log(v/1-v)\n",
    "        # This is why random_sample_soft_v() just calls random_sample_soft()        \n",
    "        logQ_v, softSamples_v = self.recognition_network(sampler=functools.partial(\n",
    "            random_sample_soft_v, temperature=self._temperature))\n",
    "        softELBO_v, _ = self.generator_network(x, softSamples_v, logQ_v)\n",
    "        logQ_v = torch.sum(logQ_v)\n",
    "        \n",
    "        gumbel_cv_learning_signal = softELBO_v.detach()\n",
    "        \n",
    "         # Gumbel CV\n",
    "        gumbel_cv = gumbel_cv_learning_signal * logQHard - softELBO + softELBO_v\n",
    "        \n",
    "        return {\n",
    "            'logQHard': logQHard,\n",
    "            'hardELBO': hardELBO,\n",
    "            'reinforce_model_grad': reinforce_model_grad,\n",
    "            'gumbel_cv': gumbel_cv\n",
    "        }   \n",
    "\n",
    "class Baseline(nn.Module):\n",
    "    def __init__(self, hparams):\n",
    "        super(Baseline, self).__init__()\n",
    "        # For centering the learning signal, from the NVIL paper (2.3.1) https://arxiv.org/pdf/1402.0030.pdf\n",
    "        # Input dependent baseline that is trained to minimize the MSE with the learning signal\n",
    "        self.out = nn.Sequential(\n",
    "           nn.linear(hparams.n_inputs, 100),\n",
    "           nn.Tanh(),\n",
    "           nn.linear(100, 1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "a bytes-like object is required, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-1b2653f6cdc3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Load MNIST dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mtrain_xs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_xs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_xs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mtrain_dataloader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataloader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_xs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'batch_size'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# mean centering on training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Workspace\\REBAR-pytorch\\rebar\\datasets.py\u001b[0m in \u001b[0;36mload_data\u001b[1;34m(hparams)\u001b[0m\n\u001b[0;32m     35\u001b[0m   \u001b[1;32melif\u001b[0m \u001b[0mhparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'task'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'omni'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0mreader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_omniglot\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m   \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbinarize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0mhparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'dynamic_b'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Workspace\\REBAR-pytorch\\rebar\\datasets.py\u001b[0m in \u001b[0;36mread_MNIST\u001b[1;34m(binarize)\u001b[0m\n\u001b[0;32m     64\u001b[0m   \"\"\"\n\u001b[0;32m     65\u001b[0m   \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMNIST_BINARIZED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m     \u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mbinarize\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: a bytes-like object is required, not 'str'"
     ]
    }
   ],
   "source": [
    "# Random seed\n",
    "random_seed = 1337\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "# Load MNIST dataset\n",
    "train_xs, val_xs, test_xs = datasets.load_data(hparams)\n",
    "train_dataloader = Dataloader(train_xs, shuffle=True, batch_size=hparams['batch_size'], num_workers=0)\n",
    "# mean centering on training data\n",
    "mean_xs = np.mean(train_xs, axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mean_xs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-b1aaae97c047>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msbn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSBNRebar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean_xs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mbaseline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBaseline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msbn_opt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msbn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'learning_rate'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mean_xs' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "sbn = SBNRebar(hparams, mean_xs)\n",
    "baseline = Baseline(hparams)\n",
    "\n",
    "sbn_opt = optim.Adam(sbn.parameters(), lr=hparams['learning_rate'])\n",
    "baseline_opt = optim.Adam(baseline.parameters(), lr=hparams['learning_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main training loop, where we compute REBAR gradients and update model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Dataloader\n",
    "for x in train:\n",
    "    sbn_outs = sbn.forward(x)\n",
    "    baseline_out = baseline.forward(x)\n",
    "    nvil_gradient = (sbn_outs['hardELBO'].detach() - baseline_out) * sbn_outs['logQHard'] \n",
    "        + sbn_outs['reinforce_model_grad']\n",
    "        \n",
    "    f_grads = grad(-nvil_gradient.mean(), sbn.parameters())\n",
    "    h_grads = # multipy h_grads by eta\n",
    "    total_grads = # add grads and vars\n",
    "    \n",
    "    # variance objective\n",
    "    \n",
    "    # baseline grads, CV grads, eta grads?\n",
    "    \n",
    "    sbn_opt.zero_grad()\n",
    "    baseline_opt.zero_grad()\n",
    "    \n",
    "    # set gradients\n",
    "    for (g, p) in zip(total_grads, sbn.parameters()):\n",
    "        p.grad = g\n",
    "    \n",
    "    sbn_opt.step()\n",
    "    baseline_opt.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
